![presentation airflow](img/airflow_logo.webp)
# Automatisation d'un Workflow ML avec Airflow et Notifications

Ce projet dÃ©ploie un pipeline de Machine Learning complet orchestrÃ© par Apache Airflow. Le workflow automatise le chargement des donnÃ©es, le prÃ©traitement, l'entraÃ®nement d'un modÃ¨le de rÃ©gression logistique, son Ã©valuation, et envoie des notifications par e-mail en cas de succÃ¨s ou d'Ã©chec.

Le projet est conÃ§u pour Ãªtre exÃ©cutÃ© localement dans un environnement de dÃ©veloppement et inclut des outils comme **MailCatcher** pour simuler l'envoi d'e-mails.

## ðŸ“‹ Contexte

Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre d'une Ã©valuation visant Ã  dÃ©montrer les compÃ©tences suivantes :
- Installation et configuration d'un environnement Apache Airflow.
- CrÃ©ation et dÃ©ploiement d'un DAG (Directed Acyclic Graph).
- Orchestration d'un workflow de Machine Learning de bout en bout.
- ImplÃ©mentation de notifications pour le monitoring.

## âœ¨ FonctionnalitÃ©s

- **Pipeline ML AutomatisÃ©** : EnchaÃ®nement des tÃ¢ches de prÃ©paration des donnÃ©es, entraÃ®nement et Ã©valuation.
- **Notifications par E-mail** : Utilisation des `callbacks` Airflow pour envoyer un e-mail en cas de succÃ¨s ou d'Ã©chec du DAG.
- **Test d'E-mail Local** : IntÃ©gration de MailCatcher pour intercepter les e-mails envoyÃ©s par Airflow sans nÃ©cessiter de serveur SMTP rÃ©el.
- **Simulation de DÃ©faillance** : Une tÃ¢che de test permet de forcer l'Ã©chec du DAG via une Variable Airflow pour tester les notifications d'erreur.
- **Scripts d'Automatisation** : Des scripts shell (`init_airflow.sh`, `run_airflow.sh`) simplifient l'installation et le lancement du projet.
- **IntÃ©gration Externe** : Le DAG notifie une API Flask externe en fin d'exÃ©cution.

## âš™ï¸ Workflow du DAG

Le DAG `Airflow_Lab2` suit la sÃ©quence de tÃ¢ches suivante :

```
[load_data] >> [data_preprocessing] >> [separate_data_outputs] >> [build_model] >> [evaluate_model] >> [test_failure] >> [notify_flask]
```

## ðŸ› ï¸ PrÃ©requis

Avant de commencer, assurez-vous d'avoir les outils suivants installÃ©s sur votre systÃ¨me (testÃ© sur Ubuntu) :

- **Python 3.12+**
- **Ruby 2.5+** et les outils de dÃ©veloppement associÃ©s (`ruby-dev`, `build-essential`)
- **SQLite3** (`libsqlite3-dev`)
- **Git**

## ðŸš€ Installation et Configuration

Suivez ces Ã©tapes pour mettre en place l'environnement.

1.  **Clonez le dÃ©pÃ´t :**
    ```bash
    git clone https://github.com/benjsant/airflow_ml_mail_automation.git
    cd airflow_ml_mail_automation
    ```

2.  **Rendez les scripts exÃ©cutables :**
    ```bash
    chmod +x init_airflow.sh run_airflow.sh
    ```

3.  **Lancez le script d'initialisation :**
    Ce script va :
    - Installer et lancer MailCatcher en arriÃ¨re-plan.
    - CrÃ©er un environnement virtuel Python (`.venv`).
    - Installer les dÃ©pendances depuis `requirements.txt`.
    - Importer les variables Airflow depuis `dags/data/variables.json`.

    ```bash
    ./init_airflow.sh
    ```

## â–¶ï¸ Lancement du Projet

Une fois l'initialisation terminÃ©e, lancez Airflow avec le script suivant :

```bash
./run_airflow.sh
```

Ce script active l'environnement virtuel et dÃ©marre Airflow en mode `standalone`, ce qui lance le webserver et le scheduler.

## ðŸ’» Comment Utiliser

1.  **AccÃ©der Ã  l'interface web d'Airflow** :
    - Ouvrez votre navigateur et allez sur `http://localhost:8080`.
    - Les identifiants par dÃ©faut pour le mode `standalone` sont `airflow` / `airflow25`.

2.  **AccÃ©der Ã  MailCatcher** :
    - Pour voir les e-mails qui seront envoyÃ©s, ouvrez un autre onglet et allez sur `http://localhost:1080`.

3.  **Activer et DÃ©clencher le DAG** :
    - Dans l'interface Airflow, trouvez le DAG nommÃ© `Airflow_Lab2`.
    - Activez-le en cliquant sur le bouton Ã  gauche de son nom.
    - Pour le lancer manuellement, cliquez sur le bouton "Play" (â–¶ï¸) Ã  droite.

4.  **Observer les RÃ©sultats** :
    - Suivez l'exÃ©cution des tÃ¢ches dans la vue "Graph" ou "Grid".
    - Une fois le DAG terminÃ© (avec succÃ¨s ou en Ã©chec), consultez l'interface de MailCatcher (`http://localhost:1080`) pour voir l'e-mail de notification.


## ðŸ”§ Configuration AvancÃ©e

### Simuler un Ã‰chec

Pour tester la notification d'Ã©chec, vous pouvez modifier la Variable Airflow `force_failure`.
- Dans l'interface Airflow, allez dans `Admin -> Variables`.
- Modifiez la variable `force_failure` et mettez sa valeur Ã  `true`.
- Relancez le DAG. La tÃ¢che `test_failure_task` Ã©chouera, ce qui dÃ©clenchera l'envoi de l'e-mail d'erreur.

## ðŸ§© IntÃ©gration Flask

Une petite API Flask est intÃ©grÃ©e au projet (dags/Flask_API.py).
Elle est utilisÃ©e pour afficher les rÃ©sultats du pipeline Airflow (succÃ¨s ou Ã©chec) et tester la communication entre Airflow et un service externe.

Lâ€™API expose les routes suivantes :

| Endpoint | Description |
|-----------|--------------|
| `/` | Redirige vers la derniÃ¨re exÃ©cution (`/success` ou `/failure`) |
| `/success` | Page de succÃ¨s aprÃ¨s exÃ©cution du DAG |
| `/failure` | Page dâ€™erreur aprÃ¨s Ã©chec du DAG |
| `/health` | VÃ©rifie que lâ€™API est active |

## ðŸ“‚ Structure du Projet

```bash
airflow_ml_mail_automation/
â”œâ”€â”€ airflow
â”‚   â”œâ”€â”€ airflow.cfg
â”‚   â”œâ”€â”€ airflow.db
â”‚   â”œâ”€â”€ logs
â”‚   â””â”€â”€ simple_auth_manager_passwords.json.generated
â”œâ”€â”€ dags
â”‚   â”œâ”€â”€ data
â”‚   â”‚   â”œâ”€â”€ advertising.csv
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ variables.json
â”‚   â”œâ”€â”€ Flask_API.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ model
â”‚   â”‚   â””â”€â”€ model.sav
â”‚   â”œâ”€â”€ src
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model_development.py
â”‚   â”œâ”€â”€ templates
â”‚   â”‚   â”œâ”€â”€ failure.html
â”‚   â”‚   â””â”€â”€ success.html
â”‚   â”œâ”€â”€ templates_flask
â”‚   â”‚   â””â”€â”€ failure.html
â”‚   â””â”€â”€ working_data
â”‚       â”œâ”€â”€ preprocessed.pkl
â”‚       â””â”€â”€ raw.pkl
â”œâ”€â”€ init_airflow.sh
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ run_airflow.sh
```
